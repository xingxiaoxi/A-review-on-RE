Discovering Relations among Named Entities from Large Corpora

add : https://www.aclweb.org/anthology/P04-1053.pdf

完全的无监督学习和半监督学习不一样，半监督学习仍然需要少量的种子实例

主要思想如下 : 1 在预料中标出命名实体
               2 找出一同出现的实体和他们的上下文
               3 计算文本的相似度
               4 对每个聚类进行标注

一对实体会出现很多次，把他们的上下文组成一个集合

对于一同出现定义为在一个句子中，并且被隔开的单词小于N

然而并没有把e1 ... e2 之外的词考虑到，这些词对于计算相似度也很重要

e1 和 e2作为实体对出现的频率较少的话，考虑到置信度应该把他们消除

用向量空间和余弦相似度来计算相似度，只计算实体类型相同的实体对

比如两者都是per-org，于是他们被称作per-org domain

向量是用tf*idf来计算，是一种根据词频乘积的比较古老的方法

主要的限制是第一个实体左侧的词和第二个实体右侧的词没有被考虑

Unsupervised Feature Selection for Relation Extraction

https://www.aclweb.org/anthology/I05-2045.pdf

但是计算相似度的时候有一些问题，存在大量噪音干扰相似度的计算

所以希望能从特征中抽取一个子特征

为了选择重要特征的子集，首先根据单词在聚类上的重要性对其进行排名。 

重要性可以通过熵标准来评估。 基于熵的特征排名是基于一个假设，即如果某个特征的存在妨碍了数据集的可分离性，则该特征是不相关的

实例相似度表示为S_i,j = exp(−α ∗ D_i,j ),D是欧拉距离

整体的相似度E为与S相关的一个等式，具体可以看论文

特征熵的计算方法是：依次从特征空间中删除每个单词，并使用等式计算新特征空间中的E。

如果将特征删除导致E最小，则特征最不重要，我们可以得到排名。

选择很多个子特征序列，然后根据一个比值来确定最恰当的子特征

比值是类外距离/类内距离

比值越高说明越好
